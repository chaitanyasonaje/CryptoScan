{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CNN + LSTM"
      ],
      "metadata": {
        "id": "NumoY7COK4Ra"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV4qTvp-YP18",
        "outputId": "6a1bdfa4-9f68-46f4-cee7-fc0c6758c107"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 170ms/step - accuracy: 0.2744 - loss: 1.7648 - val_accuracy: 0.2723 - val_loss: 1.6213\n",
            "Epoch 2/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 159ms/step - accuracy: 0.2982 - loss: 1.5785 - val_accuracy: 0.2838 - val_loss: 1.6013\n",
            "Epoch 3/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 158ms/step - accuracy: 0.2933 - loss: 1.5522 - val_accuracy: 0.2838 - val_loss: 1.5964\n",
            "Epoch 4/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 140ms/step - accuracy: 0.3091 - loss: 1.5389 - val_accuracy: 0.2540 - val_loss: 1.5961\n",
            "Epoch 5/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 155ms/step - accuracy: 0.3172 - loss: 1.5318 - val_accuracy: 0.2540 - val_loss: 1.5894\n",
            "Epoch 6/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 160ms/step - accuracy: 0.2884 - loss: 1.5396 - val_accuracy: 0.2838 - val_loss: 1.5861\n",
            "Epoch 7/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 171ms/step - accuracy: 0.2792 - loss: 1.5595 - val_accuracy: 0.2838 - val_loss: 1.5821\n",
            "Epoch 8/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 142ms/step - accuracy: 0.2898 - loss: 1.5437 - val_accuracy: 0.2540 - val_loss: 1.5854\n",
            "Epoch 9/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 142ms/step - accuracy: 0.3086 - loss: 1.5367 - val_accuracy: 0.2838 - val_loss: 1.5866\n",
            "Epoch 10/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 159ms/step - accuracy: 0.2917 - loss: 1.5462 - val_accuracy: 0.2723 - val_loss: 1.5809\n",
            "Epoch 11/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 178ms/step - accuracy: 0.2923 - loss: 1.5367 - val_accuracy: 0.2838 - val_loss: 1.5798\n",
            "Epoch 12/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 191ms/step - accuracy: 0.2887 - loss: 1.5417 - val_accuracy: 0.2838 - val_loss: 1.5775\n",
            "Epoch 13/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 155ms/step - accuracy: 0.3063 - loss: 1.5434 - val_accuracy: 0.2723 - val_loss: 1.5815\n",
            "Epoch 14/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 160ms/step - accuracy: 0.2926 - loss: 1.5357 - val_accuracy: 0.2838 - val_loss: 1.5797\n",
            "Epoch 15/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.2744 - loss: 1.5465 - val_accuracy: 0.2838 - val_loss: 1.5775\n",
            "Epoch 16/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 138ms/step - accuracy: 0.2994 - loss: 1.5153 - val_accuracy: 0.2838 - val_loss: 1.5795\n",
            "Epoch 17/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 142ms/step - accuracy: 0.3099 - loss: 1.5363 - val_accuracy: 0.2838 - val_loss: 1.5790\n",
            "Epoch 18/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 170ms/step - accuracy: 0.3102 - loss: 1.5260 - val_accuracy: 0.2838 - val_loss: 1.5783\n",
            "Epoch 19/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 157ms/step - accuracy: 0.3087 - loss: 1.5109 - val_accuracy: 0.2838 - val_loss: 1.5767\n",
            "Epoch 20/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.2986 - loss: 1.5279 - val_accuracy: 0.2838 - val_loss: 1.5763\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.2900 - loss: 1.5748\n",
            "Final Test Accuracy: 28.38%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Mount Google Drive if dataset is stored there\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Load Dataset (Update path if using Google Drive)\n",
        "file_path = \"cryptography_dataset_enhanced.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=[\"Key\"]).reset_index(drop=True)\n",
        "\n",
        "# Encode Algorithm labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Algorithm_Label\"] = label_encoder.fit_transform(df[\"Algorithm\"])\n",
        "\n",
        "# Convert Ciphertext to fixed-length ASCII representation\n",
        "def text_to_ascii(text, max_length=128):\n",
        "    ascii_vals = [ord(c) for c in text[:max_length]]\n",
        "    if len(ascii_vals) < max_length:\n",
        "        ascii_vals.extend([0] * (max_length - len(ascii_vals)))\n",
        "    return np.array(ascii_vals)\n",
        "\n",
        "ciphertext_numeric = np.array([text_to_ascii(text, 128) for text in df[\"Ciphertext\"]])\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "ciphertext_scaled = scaler.fit_transform(ciphertext_numeric)\n",
        "\n",
        "# Reshape for CNN-LSTM input\n",
        "X = ciphertext_scaled.reshape(ciphertext_scaled.shape[0], 128, 1)\n",
        "y = df[\"Algorithm_Label\"].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define CNN + LSTM Model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 1)),\n",
        "    Dropout(0.3),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    LSTM(32),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Final Test Accuracy: {test_acc * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2STyZCgsK3bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN + LSTM (V2)"
      ],
      "metadata": {
        "id": "mXQRRZrFKWtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysc06_5-cDtl",
        "outputId": "299e6990-0315-49ef-db40-5c3b73645979"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 318ms/step - accuracy: 0.2721 - loss: 1.8880 - val_accuracy: 0.4240 - val_loss: 1.2506\n",
            "Epoch 2/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 324ms/step - accuracy: 0.3989 - loss: 1.2311 - val_accuracy: 0.4340 - val_loss: 1.1706\n",
            "Epoch 3/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 289ms/step - accuracy: 0.4430 - loss: 1.1716 - val_accuracy: 0.5940 - val_loss: 0.9287\n",
            "Epoch 4/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 307ms/step - accuracy: 0.5152 - loss: 0.9780 - val_accuracy: 0.5920 - val_loss: 0.7665\n",
            "Epoch 5/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 296ms/step - accuracy: 0.5161 - loss: 0.8102 - val_accuracy: 0.5620 - val_loss: 0.7441\n",
            "Epoch 6/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 288ms/step - accuracy: 0.5287 - loss: 0.7782 - val_accuracy: 0.5620 - val_loss: 0.7395\n",
            "Epoch 7/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 320ms/step - accuracy: 0.5433 - loss: 0.7799 - val_accuracy: 0.5940 - val_loss: 0.7350\n",
            "Epoch 8/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 299ms/step - accuracy: 0.5410 - loss: 0.7445 - val_accuracy: 0.5920 - val_loss: 0.7303\n",
            "Epoch 9/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 315ms/step - accuracy: 0.5475 - loss: 0.7467 - val_accuracy: 0.5920 - val_loss: 0.7318\n",
            "Epoch 10/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 291ms/step - accuracy: 0.5538 - loss: 0.7420 - val_accuracy: 0.5340 - val_loss: 0.7383\n",
            "Epoch 11/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 293ms/step - accuracy: 0.5337 - loss: 0.7510 - val_accuracy: 0.5920 - val_loss: 0.7304\n",
            "Epoch 12/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 337ms/step - accuracy: 0.5548 - loss: 0.7493 - val_accuracy: 0.5400 - val_loss: 0.7411\n",
            "Epoch 13/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 325ms/step - accuracy: 0.5362 - loss: 0.7284 - val_accuracy: 0.5340 - val_loss: 0.7425\n",
            "Epoch 14/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 306ms/step - accuracy: 0.5443 - loss: 0.7427 - val_accuracy: 0.5620 - val_loss: 0.7384\n",
            "Epoch 15/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 318ms/step - accuracy: 0.5539 - loss: 0.7331 - val_accuracy: 0.5040 - val_loss: 0.7478\n",
            "Epoch 16/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 298ms/step - accuracy: 0.5489 - loss: 0.7415 - val_accuracy: 0.5360 - val_loss: 0.7381\n",
            "Epoch 17/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 300ms/step - accuracy: 0.5467 - loss: 0.7284 - val_accuracy: 0.5920 - val_loss: 0.7311\n",
            "Epoch 18/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 321ms/step - accuracy: 0.5734 - loss: 0.7166 - val_accuracy: 0.5920 - val_loss: 0.7334\n",
            "Epoch 19/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 301ms/step - accuracy: 0.5531 - loss: 0.7340 - val_accuracy: 0.5340 - val_loss: 0.7361\n",
            "Epoch 20/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 311ms/step - accuracy: 0.5242 - loss: 0.7322 - val_accuracy: 0.5360 - val_loss: 0.7376\n",
            "Epoch 21/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 307ms/step - accuracy: 0.5356 - loss: 0.7393 - val_accuracy: 0.5340 - val_loss: 0.7402\n",
            "Epoch 22/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 300ms/step - accuracy: 0.5300 - loss: 0.7237 - val_accuracy: 0.5940 - val_loss: 0.7351\n",
            "Epoch 23/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 307ms/step - accuracy: 0.5391 - loss: 0.7373 - val_accuracy: 0.5920 - val_loss: 0.7285\n",
            "Epoch 24/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 303ms/step - accuracy: 0.5292 - loss: 0.7402 - val_accuracy: 0.5940 - val_loss: 0.7336\n",
            "Epoch 25/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 314ms/step - accuracy: 0.5453 - loss: 0.7121 - val_accuracy: 0.5920 - val_loss: 0.7332\n",
            "Epoch 26/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 291ms/step - accuracy: 0.5329 - loss: 0.7323 - val_accuracy: 0.5940 - val_loss: 0.7343\n",
            "Epoch 27/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 313ms/step - accuracy: 0.5349 - loss: 0.7356 - val_accuracy: 0.5920 - val_loss: 0.7355\n",
            "Epoch 28/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 294ms/step - accuracy: 0.5525 - loss: 0.7225 - val_accuracy: 0.5360 - val_loss: 0.7386\n",
            "Epoch 29/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 299ms/step - accuracy: 0.5321 - loss: 0.7319 - val_accuracy: 0.5340 - val_loss: 0.7393\n",
            "Epoch 30/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 340ms/step - accuracy: 0.5341 - loss: 0.7376 - val_accuracy: 0.5360 - val_loss: 0.7385\n",
            "Epoch 31/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 324ms/step - accuracy: 0.5392 - loss: 0.7319 - val_accuracy: 0.5940 - val_loss: 0.7354\n",
            "Epoch 32/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 295ms/step - accuracy: 0.5592 - loss: 0.7174 - val_accuracy: 0.5340 - val_loss: 0.7399\n",
            "Epoch 33/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 317ms/step - accuracy: 0.5369 - loss: 0.7218 - val_accuracy: 0.5340 - val_loss: 0.7379\n",
            "Epoch 34/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 297ms/step - accuracy: 0.5507 - loss: 0.7238 - val_accuracy: 0.5340 - val_loss: 0.7412\n",
            "Epoch 35/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 314ms/step - accuracy: 0.5283 - loss: 0.7258 - val_accuracy: 0.5360 - val_loss: 0.7371\n",
            "Epoch 36/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 294ms/step - accuracy: 0.5393 - loss: 0.7218 - val_accuracy: 0.5620 - val_loss: 0.7344\n",
            "Epoch 37/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 287ms/step - accuracy: 0.5652 - loss: 0.7124 - val_accuracy: 0.5360 - val_loss: 0.7430\n",
            "Epoch 38/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 301ms/step - accuracy: 0.5527 - loss: 0.7221 - val_accuracy: 0.5920 - val_loss: 0.7380\n",
            "Epoch 39/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 295ms/step - accuracy: 0.5333 - loss: 0.7307 - val_accuracy: 0.5940 - val_loss: 0.7351\n",
            "Epoch 40/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 314ms/step - accuracy: 0.5685 - loss: 0.7110 - val_accuracy: 0.5940 - val_loss: 0.7376\n",
            "Epoch 41/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 289ms/step - accuracy: 0.5576 - loss: 0.7275 - val_accuracy: 0.5360 - val_loss: 0.7380\n",
            "Epoch 42/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 312ms/step - accuracy: 0.5352 - loss: 0.7179 - val_accuracy: 0.5620 - val_loss: 0.7385\n",
            "Epoch 43/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 297ms/step - accuracy: 0.5463 - loss: 0.7222 - val_accuracy: 0.5340 - val_loss: 0.7407\n",
            "Epoch 44/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 319ms/step - accuracy: 0.5252 - loss: 0.7442 - val_accuracy: 0.5040 - val_loss: 0.7401\n",
            "Epoch 45/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 302ms/step - accuracy: 0.5734 - loss: 0.7191 - val_accuracy: 0.5040 - val_loss: 0.7446\n",
            "Epoch 46/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 319ms/step - accuracy: 0.5429 - loss: 0.7331 - val_accuracy: 0.5940 - val_loss: 0.7356\n",
            "Epoch 47/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 324ms/step - accuracy: 0.5451 - loss: 0.7305 - val_accuracy: 0.5620 - val_loss: 0.7348\n",
            "Epoch 48/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 300ms/step - accuracy: 0.5545 - loss: 0.7136 - val_accuracy: 0.5360 - val_loss: 0.7410\n",
            "Epoch 49/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 323ms/step - accuracy: 0.5513 - loss: 0.7281 - val_accuracy: 0.5360 - val_loss: 0.7395\n",
            "Epoch 50/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 298ms/step - accuracy: 0.5469 - loss: 0.7112 - val_accuracy: 0.5360 - val_loss: 0.7456\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.5275 - loss: 0.7280\n",
            "Final Test Accuracy: 53.60%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Bidirectional\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "# Load Dataset\n",
        "file_path = \"cryptography_dataset_enhanced.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=[\"Ciphertext\"]).reset_index(drop=True)\n",
        "\n",
        "# Encode Algorithm labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Algorithm_Label\"] = label_encoder.fit_transform(df[\"Algorithm\"])\n",
        "\n",
        "# Convert Ciphertext to N-gram frequency representation\n",
        "def ngram_frequencies(text, n=3, max_length=128):\n",
        "    text = text[:max_length]  # Truncate if needed\n",
        "    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]  # Generate n-grams\n",
        "    freq = Counter(ngrams)  # Count occurrences\n",
        "    vector = np.zeros(max_length)  # Fixed-length vector\n",
        "    for i, (gram, count) in enumerate(freq.items()):\n",
        "        if i < max_length:\n",
        "            vector[i] = count  # Assign frequency\n",
        "    return vector\n",
        "\n",
        "ciphertext_features = np.array([ngram_frequencies(text, n=3) for text in df[\"Ciphertext\"]])\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "ciphertext_scaled = scaler.fit_transform(ciphertext_features)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X = ciphertext_scaled.reshape(ciphertext_scaled.shape[0], 128, 1)\n",
        "y = df[\"Algorithm_Label\"].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define CNN + BiLSTM Model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(128, 1)),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Final Test Accuracy: {test_acc * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN , LSTM , CNN+LSTM"
      ],
      "metadata": {
        "id": "DTmE3PPDKm7V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXKGkLhXvCks",
        "outputId": "c7d3c289-86d5-4f95-8123-7bca1771af8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model CNN-LSTM on Part 1 of dataset\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 134ms/step - accuracy: 0.2713 - loss: 1.7839 - val_accuracy: 0.3066 - val_loss: 1.5547\n",
            "Epoch 2/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 134ms/step - accuracy: 0.2612 - loss: 1.6255 - val_accuracy: 0.2930 - val_loss: 1.5370\n",
            "Epoch 3/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 136ms/step - accuracy: 0.2801 - loss: 1.5886 - val_accuracy: 0.3047 - val_loss: 1.5338\n",
            "Epoch 4/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 135ms/step - accuracy: 0.2752 - loss: 1.5809 - val_accuracy: 0.2930 - val_loss: 1.5233\n",
            "Epoch 5/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 116ms/step - accuracy: 0.2817 - loss: 1.5885 - val_accuracy: 0.2812 - val_loss: 1.5190\n",
            "Epoch 6/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 132ms/step - accuracy: 0.2831 - loss: 1.5701 - val_accuracy: 0.2930 - val_loss: 1.5170\n",
            "Epoch 7/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 136ms/step - accuracy: 0.2637 - loss: 1.5762 - val_accuracy: 0.2930 - val_loss: 1.5159\n",
            "Epoch 8/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 136ms/step - accuracy: 0.3011 - loss: 1.5365 - val_accuracy: 0.4277 - val_loss: 1.3293\n",
            "Epoch 9/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.4735 - loss: 1.1788 - val_accuracy: 0.5703 - val_loss: 0.7060\n",
            "Epoch 10/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 117ms/step - accuracy: 0.5717 - loss: 0.7771 - val_accuracy: 0.5840 - val_loss: 0.7311\n",
            "Epoch 11/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 135ms/step - accuracy: 0.5581 - loss: 0.7754 - val_accuracy: 0.5859 - val_loss: 0.7131\n",
            "Epoch 12/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 136ms/step - accuracy: 0.5782 - loss: 0.7439 - val_accuracy: 0.5781 - val_loss: 0.7229\n",
            "Epoch 13/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 136ms/step - accuracy: 0.5310 - loss: 0.7487 - val_accuracy: 0.5840 - val_loss: 0.6886\n",
            "Epoch 14/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 120ms/step - accuracy: 0.5578 - loss: 0.7101 - val_accuracy: 0.5879 - val_loss: 0.6725\n",
            "Epoch 15/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 135ms/step - accuracy: 0.5392 - loss: 0.7118 - val_accuracy: 0.5938 - val_loss: 0.6715\n",
            "Epoch 16/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 137ms/step - accuracy: 0.5674 - loss: 0.7173 - val_accuracy: 0.5938 - val_loss: 0.6825\n",
            "Epoch 17/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 120ms/step - accuracy: 0.4323 - loss: 1.2979 - val_accuracy: 0.2910 - val_loss: 1.5195\n",
            "Epoch 18/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 119ms/step - accuracy: 0.2691 - loss: 1.5946 - val_accuracy: 0.2930 - val_loss: 1.5156\n",
            "Epoch 19/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 136ms/step - accuracy: 0.2720 - loss: 1.5615 - val_accuracy: 0.2812 - val_loss: 1.5167\n",
            "Epoch 20/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 136ms/step - accuracy: 0.2809 - loss: 1.5811 - val_accuracy: 0.2812 - val_loss: 1.5203\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2710 - loss: 1.5150\n",
            "Final Test Accuracy for CNN-LSTM: 28.12%\n",
            "\n",
            "Training Model LSTM on Part 2 of dataset\n",
            "Epoch 1/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.2600 - loss: 1.7653 - val_accuracy: 0.2754 - val_loss: 1.5832\n",
            "Epoch 2/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 117ms/step - accuracy: 0.2755 - loss: 1.5890 - val_accuracy: 0.2930 - val_loss: 1.5575\n",
            "Epoch 3/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.2806 - loss: 1.5539 - val_accuracy: 0.2754 - val_loss: 1.5538\n",
            "Epoch 4/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 101ms/step - accuracy: 0.2841 - loss: 1.5594 - val_accuracy: 0.2754 - val_loss: 1.5533\n",
            "Epoch 5/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 100ms/step - accuracy: 0.2762 - loss: 1.5747 - val_accuracy: 0.3027 - val_loss: 1.5442\n",
            "Epoch 6/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - accuracy: 0.3085 - loss: 1.5295 - val_accuracy: 0.4258 - val_loss: 1.2328\n",
            "Epoch 7/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 113ms/step - accuracy: 0.3418 - loss: 1.4701 - val_accuracy: 0.2930 - val_loss: 1.5509\n",
            "Epoch 8/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 117ms/step - accuracy: 0.2828 - loss: 1.5680 - val_accuracy: 0.3027 - val_loss: 1.5449\n",
            "Epoch 9/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 122ms/step - accuracy: 0.2951 - loss: 1.5356 - val_accuracy: 0.4551 - val_loss: 1.4422\n",
            "Epoch 10/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.3950 - loss: 1.5404 - val_accuracy: 0.5684 - val_loss: 1.1692\n",
            "Epoch 11/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 117ms/step - accuracy: 0.5137 - loss: 1.0815 - val_accuracy: 0.5859 - val_loss: 0.7281\n",
            "Epoch 12/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 101ms/step - accuracy: 0.5396 - loss: 0.7816 - val_accuracy: 0.6016 - val_loss: 0.6769\n",
            "Epoch 13/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 101ms/step - accuracy: 0.5656 - loss: 0.7129 - val_accuracy: 0.5898 - val_loss: 0.6756\n",
            "Epoch 14/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 101ms/step - accuracy: 0.5829 - loss: 0.7030 - val_accuracy: 0.5781 - val_loss: 0.6721\n",
            "Epoch 15/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 117ms/step - accuracy: 0.5802 - loss: 0.6749 - val_accuracy: 0.5898 - val_loss: 0.6664\n",
            "Epoch 16/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.5541 - loss: 0.7025 - val_accuracy: 0.5898 - val_loss: 0.6656\n",
            "Epoch 17/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 97ms/step - accuracy: 0.5648 - loss: 0.6923 - val_accuracy: 0.5547 - val_loss: 0.6659\n",
            "Epoch 18/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 101ms/step - accuracy: 0.5717 - loss: 0.6761 - val_accuracy: 0.5781 - val_loss: 0.6654\n",
            "Epoch 19/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 100ms/step - accuracy: 0.5796 - loss: 0.6699 - val_accuracy: 0.5898 - val_loss: 0.6724\n",
            "Epoch 20/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 111ms/step - accuracy: 0.5888 - loss: 0.6721 - val_accuracy: 0.5664 - val_loss: 0.6746\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5421 - loss: 0.6798\n",
            "Final Test Accuracy for LSTM: 56.64%\n",
            "\n",
            "Training Model CNN on Part 3 of dataset\n",
            "Epoch 1/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.4744 - loss: 1.2848 - val_accuracy: 0.5898 - val_loss: 0.7253\n",
            "Epoch 2/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5916 - loss: 0.8046 - val_accuracy: 0.5469 - val_loss: 0.7006\n",
            "Epoch 3/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5912 - loss: 0.7477 - val_accuracy: 0.5488 - val_loss: 0.6928\n",
            "Epoch 4/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6179 - loss: 0.7031 - val_accuracy: 0.5410 - val_loss: 0.6982\n",
            "Epoch 5/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6473 - loss: 0.6582 - val_accuracy: 0.5957 - val_loss: 0.6838\n",
            "Epoch 6/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6263 - loss: 0.6626 - val_accuracy: 0.5723 - val_loss: 0.6846\n",
            "Epoch 7/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6315 - loss: 0.6698 - val_accuracy: 0.5957 - val_loss: 0.6879\n",
            "Epoch 8/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6680 - loss: 0.6414 - val_accuracy: 0.5840 - val_loss: 0.6925\n",
            "Epoch 9/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6775 - loss: 0.6225 - val_accuracy: 0.5879 - val_loss: 0.6994\n",
            "Epoch 10/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6687 - loss: 0.6289 - val_accuracy: 0.5879 - val_loss: 0.6998\n",
            "Epoch 11/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6887 - loss: 0.6019 - val_accuracy: 0.5801 - val_loss: 0.6992\n",
            "Epoch 12/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6951 - loss: 0.5977 - val_accuracy: 0.5957 - val_loss: 0.7144\n",
            "Epoch 13/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7073 - loss: 0.5735 - val_accuracy: 0.5840 - val_loss: 0.7042\n",
            "Epoch 14/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7162 - loss: 0.5828 - val_accuracy: 0.5820 - val_loss: 0.7046\n",
            "Epoch 15/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6979 - loss: 0.5801 - val_accuracy: 0.6055 - val_loss: 0.7285\n",
            "Epoch 16/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7524 - loss: 0.5226 - val_accuracy: 0.5957 - val_loss: 0.7340\n",
            "Epoch 17/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7294 - loss: 0.5388 - val_accuracy: 0.5742 - val_loss: 0.7418\n",
            "Epoch 18/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7405 - loss: 0.5338 - val_accuracy: 0.6094 - val_loss: 0.7348\n",
            "Epoch 19/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7519 - loss: 0.5105 - val_accuracy: 0.5840 - val_loss: 0.7612\n",
            "Epoch 20/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7541 - loss: 0.5054 - val_accuracy: 0.5781 - val_loss: 0.7625\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5771 - loss: 0.8092 \n",
            "Final Test Accuracy for CNN: 57.81%\n",
            "\n",
            "Training Model MLP on Part 4 of dataset\n",
            "Epoch 1/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3654 - loss: 1.5295 - val_accuracy: 0.5957 - val_loss: 0.7560\n",
            "Epoch 2/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6043 - loss: 0.7714 - val_accuracy: 0.5762 - val_loss: 0.6861\n",
            "Epoch 3/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6186 - loss: 0.6933 - val_accuracy: 0.5801 - val_loss: 0.6806\n",
            "Epoch 4/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6019 - loss: 0.6927 - val_accuracy: 0.5625 - val_loss: 0.6840\n",
            "Epoch 5/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6273 - loss: 0.6688 - val_accuracy: 0.5840 - val_loss: 0.6792\n",
            "Epoch 6/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6150 - loss: 0.6689 - val_accuracy: 0.5801 - val_loss: 0.6816\n",
            "Epoch 7/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6380 - loss: 0.6700 - val_accuracy: 0.5684 - val_loss: 0.7021\n",
            "Epoch 8/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6638 - loss: 0.6420 - val_accuracy: 0.5957 - val_loss: 0.6763\n",
            "Epoch 9/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6704 - loss: 0.6377 - val_accuracy: 0.5508 - val_loss: 0.6886\n",
            "Epoch 10/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6534 - loss: 0.6565 - val_accuracy: 0.6055 - val_loss: 0.6761\n",
            "Epoch 11/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6454 - loss: 0.6405 - val_accuracy: 0.5527 - val_loss: 0.6885\n",
            "Epoch 12/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6584 - loss: 0.6181 - val_accuracy: 0.5684 - val_loss: 0.6807\n",
            "Epoch 13/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6629 - loss: 0.6304 - val_accuracy: 0.5703 - val_loss: 0.6953\n",
            "Epoch 14/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6665 - loss: 0.6326 - val_accuracy: 0.5566 - val_loss: 0.7007\n",
            "Epoch 15/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6878 - loss: 0.6092 - val_accuracy: 0.5527 - val_loss: 0.7010\n",
            "Epoch 16/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6935 - loss: 0.6105 - val_accuracy: 0.5898 - val_loss: 0.6905\n",
            "Epoch 17/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7074 - loss: 0.5953 - val_accuracy: 0.5625 - val_loss: 0.7046\n",
            "Epoch 18/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7152 - loss: 0.5928 - val_accuracy: 0.5684 - val_loss: 0.7062\n",
            "Epoch 19/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7204 - loss: 0.5834 - val_accuracy: 0.5664 - val_loss: 0.7204\n",
            "Epoch 20/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7088 - loss: 0.5905 - val_accuracy: 0.5742 - val_loss: 0.7127\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5952 - loss: 0.6778 \n",
            "Final Test Accuracy for MLP: 57.42%\n",
            "\n",
            "Training Model GRU on Part 5 of dataset\n",
            "Epoch 1/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 133ms/step - accuracy: 0.2813 - loss: 1.7691 - val_accuracy: 0.2637 - val_loss: 1.6140\n",
            "Epoch 2/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 0.2732 - loss: 1.6150 - val_accuracy: 0.2559 - val_loss: 1.5869\n",
            "Epoch 3/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 140ms/step - accuracy: 0.2990 - loss: 1.5956 - val_accuracy: 0.3008 - val_loss: 1.5728\n",
            "Epoch 4/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 169ms/step - accuracy: 0.2972 - loss: 1.5709 - val_accuracy: 0.2559 - val_loss: 1.5764\n",
            "Epoch 5/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 126ms/step - accuracy: 0.2902 - loss: 1.5707 - val_accuracy: 0.2559 - val_loss: 1.5683\n",
            "Epoch 6/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 131ms/step - accuracy: 0.2763 - loss: 1.5646 - val_accuracy: 0.3008 - val_loss: 1.5606\n",
            "Epoch 7/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 142ms/step - accuracy: 0.2998 - loss: 1.5417 - val_accuracy: 0.2637 - val_loss: 1.5582\n",
            "Epoch 8/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 142ms/step - accuracy: 0.2963 - loss: 1.5535 - val_accuracy: 0.3008 - val_loss: 1.5572\n",
            "Epoch 9/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 142ms/step - accuracy: 0.2799 - loss: 1.5638 - val_accuracy: 0.2969 - val_loss: 1.5521\n",
            "Epoch 10/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 126ms/step - accuracy: 0.2875 - loss: 1.5662 - val_accuracy: 0.2559 - val_loss: 1.5573\n",
            "Epoch 11/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.2939 - loss: 1.5480 - val_accuracy: 0.2559 - val_loss: 1.5531\n",
            "Epoch 12/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 134ms/step - accuracy: 0.2768 - loss: 1.5655 - val_accuracy: 0.2559 - val_loss: 1.5502\n",
            "Epoch 13/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step - accuracy: 0.2797 - loss: 1.5546 - val_accuracy: 0.2559 - val_loss: 1.5467\n",
            "Epoch 14/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - accuracy: 0.2716 - loss: 1.5524 - val_accuracy: 0.2559 - val_loss: 1.5505\n",
            "Epoch 15/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 127ms/step - accuracy: 0.2813 - loss: 1.5628 - val_accuracy: 0.2969 - val_loss: 1.5462\n",
            "Epoch 16/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.2722 - loss: 1.5559 - val_accuracy: 0.2559 - val_loss: 1.5473\n",
            "Epoch 17/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 142ms/step - accuracy: 0.2908 - loss: 1.5372 - val_accuracy: 0.2637 - val_loss: 1.5485\n",
            "Epoch 18/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 143ms/step - accuracy: 0.2846 - loss: 1.5621 - val_accuracy: 0.3008 - val_loss: 1.5485\n",
            "Epoch 19/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 141ms/step - accuracy: 0.3070 - loss: 1.5357 - val_accuracy: 0.3008 - val_loss: 1.5455\n",
            "Epoch 20/20\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 126ms/step - accuracy: 0.2853 - loss: 1.5541 - val_accuracy: 0.3008 - val_loss: 1.5476\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.3045 - loss: 1.5667\n",
            "Final Test Accuracy for GRU: 30.08%\n",
            "\n",
            "Best performing model: CNN with accuracy 57.81%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, GRU, Dense, Dropout, Flatten\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Dataset\n",
        "file_path = \"cryptography_dataset_enhanced.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=[\"Key\"]).reset_index(drop=True)\n",
        "\n",
        "# Encode Algorithm labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Algorithm_Label\"] = label_encoder.fit_transform(df[\"Algorithm\"])\n",
        "\n",
        "# Convert Ciphertext to fixed-length ASCII representation\n",
        "def text_to_ascii(text, max_length=128):\n",
        "    ascii_vals = [ord(c) for c in text[:max_length]]\n",
        "    if len(ascii_vals) < max_length:\n",
        "        ascii_vals.extend([0] * (max_length - len(ascii_vals)))\n",
        "    return np.array(ascii_vals)\n",
        "\n",
        "ciphertext_numeric = np.array([text_to_ascii(text, 128) for text in df[\"Ciphertext\"]])\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "ciphertext_scaled = scaler.fit_transform(ciphertext_numeric)\n",
        "\n",
        "# Reshape for CNN/LSTM input\n",
        "X = ciphertext_scaled.reshape(ciphertext_scaled.shape[0], 128, 1)\n",
        "y = df[\"Algorithm_Label\"].values\n",
        "\n",
        "# Split dataset into five parts\n",
        "split_size = len(df) // 5\n",
        "X_splits = [X[i * split_size:(i + 1) * split_size] for i in range(5)]\n",
        "y_splits = [y[i * split_size:(i + 1) * split_size] for i in range(5)]\n",
        "\n",
        "# Define different models\n",
        "def build_cnn_lstm():\n",
        "    model = Sequential([\n",
        "        Conv1D(64, kernel_size=3, activation='relu', input_shape=(128, 1)),\n",
        "        Dropout(0.3),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        LSTM(32),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_lstm():\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=(128, 1)),\n",
        "        LSTM(32),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_cnn():\n",
        "    model = Sequential([\n",
        "        Conv1D(64, kernel_size=3, activation='relu', input_shape=(128, 1)),\n",
        "        Flatten(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_mlp():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(128, 1)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_gru():\n",
        "    model = Sequential([\n",
        "        GRU(64, return_sequences=True, input_shape=(128, 1)),\n",
        "        GRU(32),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "models = {\n",
        "    \"CNN-LSTM\": build_cnn_lstm(),\n",
        "    \"LSTM\": build_lstm(),\n",
        "    \"CNN\": build_cnn(),\n",
        "    \"MLP\": build_mlp(),\n",
        "    \"GRU\": build_gru()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "results = {}\n",
        "\n",
        "for i, (X_part, y_part) in enumerate(zip(X_splits, y_splits)):\n",
        "    print(f\"\\nTraining Model {list(models.keys())[i]} on Part {i+1} of dataset\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_part, y_part, test_size=0.2, random_state=42)\n",
        "\n",
        "    model_name = list(models.keys())[i]\n",
        "    model = models[model_name]\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "    results[model_name] = test_acc\n",
        "    print(f'Final Test Accuracy for {model_name}: {test_acc * 100:.2f}%')\n",
        "\n",
        "# Display best model\n",
        "best_model = max(results, key=results.get)\n",
        "print(f'\\nBest performing model: {best_model} with accuracy {results[best_model] * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHWFiZRcB2cj",
        "outputId": "af2e5adb-c074-4530-cbf9-b482be050e54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model CNN-LSTM on Part 1 of dataset\n",
            "Epoch 1/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 713ms/step - accuracy: 0.2927 - loss: 1.7184 - val_accuracy: 0.2916 - val_loss: 1.8183\n",
            "Epoch 2/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 660ms/step - accuracy: 0.4944 - loss: 1.0424 - val_accuracy: 0.1735 - val_loss: 3.8069\n",
            "Epoch 3/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 660ms/step - accuracy: 0.5389 - loss: 0.8920 - val_accuracy: 0.4481 - val_loss: 2.0388\n",
            "Epoch 4/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 640ms/step - accuracy: 0.5579 - loss: 0.7734 - val_accuracy: 0.4381 - val_loss: 2.3279\n",
            "Epoch 5/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 655ms/step - accuracy: 0.5725 - loss: 0.7413 - val_accuracy: 0.4438 - val_loss: 2.4182\n",
            "Epoch 6/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 717ms/step - accuracy: 0.5641 - loss: 0.7395 - val_accuracy: 0.4865 - val_loss: 1.6353\n",
            "Epoch 7/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 634ms/step - accuracy: 0.5329 - loss: 0.7524 - val_accuracy: 0.4708 - val_loss: 1.9577\n",
            "Epoch 8/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 653ms/step - accuracy: 0.5556 - loss: 0.7365 - val_accuracy: 0.5548 - val_loss: 0.7528\n",
            "Epoch 9/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 654ms/step - accuracy: 0.5547 - loss: 0.7248 - val_accuracy: 0.5733 - val_loss: 0.6864\n",
            "Epoch 10/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 661ms/step - accuracy: 0.5883 - loss: 0.6919 - val_accuracy: 0.5789 - val_loss: 0.6630\n",
            "Epoch 11/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 636ms/step - accuracy: 0.5625 - loss: 0.7104 - val_accuracy: 0.5619 - val_loss: 0.6594\n",
            "Epoch 12/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 640ms/step - accuracy: 0.5650 - loss: 0.7138 - val_accuracy: 0.5775 - val_loss: 0.6632\n",
            "Epoch 13/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 656ms/step - accuracy: 0.5747 - loss: 0.6853 - val_accuracy: 0.5647 - val_loss: 0.6555\n",
            "Epoch 14/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 711ms/step - accuracy: 0.5581 - loss: 0.6970 - val_accuracy: 0.5676 - val_loss: 0.6592\n",
            "Epoch 15/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 652ms/step - accuracy: 0.5700 - loss: 0.7022 - val_accuracy: 0.5733 - val_loss: 0.6604\n",
            "Epoch 16/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 656ms/step - accuracy: 0.5927 - loss: 0.6730 - val_accuracy: 0.5789 - val_loss: 0.6547\n",
            "Epoch 17/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 658ms/step - accuracy: 0.5557 - loss: 0.6965 - val_accuracy: 0.5789 - val_loss: 0.6558\n",
            "Epoch 18/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 657ms/step - accuracy: 0.5615 - loss: 0.6941 - val_accuracy: 0.5903 - val_loss: 0.6547\n",
            "Epoch 19/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 653ms/step - accuracy: 0.5634 - loss: 0.6969 - val_accuracy: 0.5946 - val_loss: 0.6555\n",
            "Epoch 20/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 651ms/step - accuracy: 0.5552 - loss: 0.6903 - val_accuracy: 0.5818 - val_loss: 0.6541\n",
            "Epoch 21/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 648ms/step - accuracy: 0.5638 - loss: 0.6964 - val_accuracy: 0.5832 - val_loss: 0.6557\n",
            "Epoch 22/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 702ms/step - accuracy: 0.5578 - loss: 0.6925 - val_accuracy: 0.5889 - val_loss: 0.6544\n",
            "Epoch 23/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 682ms/step - accuracy: 0.5818 - loss: 0.6785 - val_accuracy: 0.4481 - val_loss: 1.6856\n",
            "Epoch 24/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 635ms/step - accuracy: 0.5119 - loss: 1.0966 - val_accuracy: 0.5320 - val_loss: 0.9676\n",
            "Epoch 25/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 704ms/step - accuracy: 0.5615 - loss: 0.7173 - val_accuracy: 0.5846 - val_loss: 0.6621\n",
            "Epoch 26/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 635ms/step - accuracy: 0.5775 - loss: 0.6966 - val_accuracy: 0.5690 - val_loss: 0.6636\n",
            "Epoch 27/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 644ms/step - accuracy: 0.5556 - loss: 0.6890 - val_accuracy: 0.5789 - val_loss: 0.6553\n",
            "Epoch 28/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 642ms/step - accuracy: 0.5602 - loss: 0.6879 - val_accuracy: 0.5889 - val_loss: 0.6539\n",
            "Epoch 29/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 717ms/step - accuracy: 0.5687 - loss: 0.6854 - val_accuracy: 0.5917 - val_loss: 0.6540\n",
            "Epoch 30/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 653ms/step - accuracy: 0.5619 - loss: 0.6873 - val_accuracy: 0.6017 - val_loss: 0.6542\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6172 - loss: 0.6264\n",
            "Final Test Accuracy for CNN-LSTM: 60.17%\n",
            "\n",
            "Training Model LSTM on Part 2 of dataset\n",
            "Epoch 1/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 555ms/step - accuracy: 0.3004 - loss: 1.7849 - val_accuracy: 0.4680 - val_loss: 1.2734\n",
            "Epoch 2/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 575ms/step - accuracy: 0.4876 - loss: 1.1902 - val_accuracy: 0.5149 - val_loss: 1.1131\n",
            "Epoch 3/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 576ms/step - accuracy: 0.5506 - loss: 0.9948 - val_accuracy: 0.5633 - val_loss: 0.7382\n",
            "Epoch 4/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 574ms/step - accuracy: 0.5716 - loss: 0.7747 - val_accuracy: 0.5661 - val_loss: 0.7140\n",
            "Epoch 5/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 577ms/step - accuracy: 0.5839 - loss: 0.7585 - val_accuracy: 0.5761 - val_loss: 0.6781\n",
            "Epoch 6/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 591ms/step - accuracy: 0.5712 - loss: 0.7271 - val_accuracy: 0.6060 - val_loss: 0.6810\n",
            "Epoch 7/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 583ms/step - accuracy: 0.5668 - loss: 0.7431 - val_accuracy: 0.5676 - val_loss: 0.7437\n",
            "Epoch 8/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 603ms/step - accuracy: 0.5703 - loss: 0.7259 - val_accuracy: 0.5633 - val_loss: 0.6701\n",
            "Epoch 9/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 555ms/step - accuracy: 0.5884 - loss: 0.6948 - val_accuracy: 0.5761 - val_loss: 0.6687\n",
            "Epoch 10/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 576ms/step - accuracy: 0.5875 - loss: 0.6894 - val_accuracy: 0.5832 - val_loss: 0.6724\n",
            "Epoch 11/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 552ms/step - accuracy: 0.5664 - loss: 0.6983 - val_accuracy: 0.5789 - val_loss: 0.6651\n",
            "Epoch 12/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 588ms/step - accuracy: 0.5772 - loss: 0.6996 - val_accuracy: 0.5704 - val_loss: 0.6672\n",
            "Epoch 13/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 548ms/step - accuracy: 0.5669 - loss: 0.7063 - val_accuracy: 0.5832 - val_loss: 0.6638\n",
            "Epoch 14/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 552ms/step - accuracy: 0.5846 - loss: 0.6724 - val_accuracy: 0.5832 - val_loss: 0.6612\n",
            "Epoch 15/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 603ms/step - accuracy: 0.5607 - loss: 0.6967 - val_accuracy: 0.5818 - val_loss: 0.6676\n",
            "Epoch 16/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 555ms/step - accuracy: 0.5894 - loss: 0.6789 - val_accuracy: 0.5789 - val_loss: 0.6614\n",
            "Epoch 17/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 599ms/step - accuracy: 0.5885 - loss: 0.6722 - val_accuracy: 0.5690 - val_loss: 0.6582\n",
            "Epoch 18/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 573ms/step - accuracy: 0.5630 - loss: 0.6816 - val_accuracy: 0.5704 - val_loss: 0.6679\n",
            "Epoch 19/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 572ms/step - accuracy: 0.5704 - loss: 0.7311 - val_accuracy: 0.5804 - val_loss: 0.7064\n",
            "Epoch 20/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 547ms/step - accuracy: 0.5641 - loss: 0.7125 - val_accuracy: 0.5718 - val_loss: 0.6847\n",
            "Epoch 21/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 556ms/step - accuracy: 0.5792 - loss: 0.6965 - val_accuracy: 0.5704 - val_loss: 0.6570\n",
            "Epoch 22/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 633ms/step - accuracy: 0.5805 - loss: 0.6675 - val_accuracy: 0.5747 - val_loss: 0.6556\n",
            "Epoch 23/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 576ms/step - accuracy: 0.5675 - loss: 0.6809 - val_accuracy: 0.5789 - val_loss: 0.6553\n",
            "Epoch 24/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 558ms/step - accuracy: 0.5589 - loss: 0.6806 - val_accuracy: 0.5718 - val_loss: 0.6641\n",
            "Epoch 25/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 566ms/step - accuracy: 0.5621 - loss: 0.6892 - val_accuracy: 0.5647 - val_loss: 0.6582\n",
            "Epoch 26/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 553ms/step - accuracy: 0.5813 - loss: 0.6668 - val_accuracy: 0.5903 - val_loss: 0.6544\n",
            "Epoch 27/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 571ms/step - accuracy: 0.5753 - loss: 0.6671 - val_accuracy: 0.5619 - val_loss: 0.6545\n",
            "Epoch 28/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 554ms/step - accuracy: 0.5759 - loss: 0.6888 - val_accuracy: 0.5804 - val_loss: 0.6546\n",
            "Epoch 29/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 573ms/step - accuracy: 0.5705 - loss: 0.6699 - val_accuracy: 0.5804 - val_loss: 0.6557\n",
            "Epoch 30/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 554ms/step - accuracy: 0.5900 - loss: 0.6764 - val_accuracy: 0.5718 - val_loss: 0.6550\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.5148 - loss: 0.7031\n",
            "Final Test Accuracy for LSTM: 57.18%\n",
            "\n",
            "Training Model CNN on Part 3 of dataset\n",
            "Epoch 1/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.4728 - loss: 1.2014 - val_accuracy: 0.5690 - val_loss: 1.3674\n",
            "Epoch 2/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.5901 - loss: 0.7259 - val_accuracy: 0.5818 - val_loss: 1.2452\n",
            "Epoch 3/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.6080 - loss: 0.6982 - val_accuracy: 0.5903 - val_loss: 1.1832\n",
            "Epoch 4/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.6380 - loss: 0.6788 - val_accuracy: 0.5832 - val_loss: 1.0417\n",
            "Epoch 5/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.6489 - loss: 0.6530 - val_accuracy: 0.5690 - val_loss: 0.9957\n",
            "Epoch 6/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.7047 - loss: 0.6266 - val_accuracy: 0.5761 - val_loss: 0.8806\n",
            "Epoch 7/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.6933 - loss: 0.6108 - val_accuracy: 0.5818 - val_loss: 0.8034\n",
            "Epoch 8/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step - accuracy: 0.7229 - loss: 0.5679 - val_accuracy: 0.5974 - val_loss: 0.7555\n",
            "Epoch 9/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - accuracy: 0.7410 - loss: 0.5460 - val_accuracy: 0.5818 - val_loss: 0.7273\n",
            "Epoch 10/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - accuracy: 0.7743 - loss: 0.5050 - val_accuracy: 0.5804 - val_loss: 0.6977\n",
            "Epoch 11/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.7698 - loss: 0.4994 - val_accuracy: 0.5974 - val_loss: 0.6915\n",
            "Epoch 12/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.8156 - loss: 0.4494 - val_accuracy: 0.5718 - val_loss: 0.7387\n",
            "Epoch 13/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.8132 - loss: 0.4301 - val_accuracy: 0.5704 - val_loss: 0.7494\n",
            "Epoch 14/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8362 - loss: 0.3930 - val_accuracy: 0.5647 - val_loss: 0.7922\n",
            "Epoch 15/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.8493 - loss: 0.3704 - val_accuracy: 0.5861 - val_loss: 0.8088\n",
            "Epoch 16/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 64ms/step - accuracy: 0.8673 - loss: 0.3260 - val_accuracy: 0.5818 - val_loss: 0.8542\n",
            "Epoch 17/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 80ms/step - accuracy: 0.8668 - loss: 0.3068 - val_accuracy: 0.5804 - val_loss: 0.8864\n",
            "Epoch 18/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.8941 - loss: 0.2852 - val_accuracy: 0.5775 - val_loss: 0.9936\n",
            "Epoch 19/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.8970 - loss: 0.2580 - val_accuracy: 0.5846 - val_loss: 0.9686\n",
            "Epoch 20/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.9014 - loss: 0.2557 - val_accuracy: 0.5747 - val_loss: 1.0271\n",
            "Epoch 21/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9195 - loss: 0.2284 - val_accuracy: 0.5718 - val_loss: 1.0278\n",
            "Epoch 22/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 62ms/step - accuracy: 0.9252 - loss: 0.2113 - val_accuracy: 0.5676 - val_loss: 1.1862\n",
            "Epoch 23/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - accuracy: 0.9153 - loss: 0.2170 - val_accuracy: 0.5718 - val_loss: 1.2346\n",
            "Epoch 24/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9353 - loss: 0.1706 - val_accuracy: 0.5747 - val_loss: 1.1988\n",
            "Epoch 25/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9362 - loss: 0.1727 - val_accuracy: 0.5889 - val_loss: 1.1932\n",
            "Epoch 26/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.9384 - loss: 0.1590 - val_accuracy: 0.5761 - val_loss: 1.2568\n",
            "Epoch 27/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - accuracy: 0.9456 - loss: 0.1478 - val_accuracy: 0.5619 - val_loss: 1.3758\n",
            "Epoch 28/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 61ms/step - accuracy: 0.9436 - loss: 0.1571 - val_accuracy: 0.5804 - val_loss: 1.3841\n",
            "Epoch 29/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step - accuracy: 0.9436 - loss: 0.1417 - val_accuracy: 0.5775 - val_loss: 1.3702\n",
            "Epoch 30/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - accuracy: 0.9480 - loss: 0.1469 - val_accuracy: 0.5818 - val_loss: 1.3514\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5505 - loss: 1.4291\n",
            "Final Test Accuracy for CNN: 58.18%\n",
            "\n",
            "Training Model MLP on Part 4 of dataset\n",
            "Epoch 1/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.2676 - loss: 2.1868 - val_accuracy: 0.5619 - val_loss: 1.1041\n",
            "Epoch 2/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5688 - loss: 0.9700 - val_accuracy: 0.5832 - val_loss: 0.8152\n",
            "Epoch 3/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5926 - loss: 0.8019 - val_accuracy: 0.5889 - val_loss: 0.7232\n",
            "Epoch 4/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5950 - loss: 0.7398 - val_accuracy: 0.5733 - val_loss: 0.6977\n",
            "Epoch 5/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6028 - loss: 0.7152 - val_accuracy: 0.5946 - val_loss: 0.6882\n",
            "Epoch 6/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6504 - loss: 0.6554 - val_accuracy: 0.5861 - val_loss: 0.6845\n",
            "Epoch 7/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6485 - loss: 0.6551 - val_accuracy: 0.6017 - val_loss: 0.6726\n",
            "Epoch 8/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6480 - loss: 0.6552 - val_accuracy: 0.5960 - val_loss: 0.6782\n",
            "Epoch 9/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6598 - loss: 0.6519 - val_accuracy: 0.5846 - val_loss: 0.6844\n",
            "Epoch 10/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6446 - loss: 0.6502 - val_accuracy: 0.5747 - val_loss: 0.6777\n",
            "Epoch 11/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6620 - loss: 0.6287 - val_accuracy: 0.5804 - val_loss: 0.6834\n",
            "Epoch 12/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6793 - loss: 0.6174 - val_accuracy: 0.5804 - val_loss: 0.6911\n",
            "Epoch 13/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6884 - loss: 0.6055 - val_accuracy: 0.5775 - val_loss: 0.6841\n",
            "Epoch 14/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6933 - loss: 0.5925 - val_accuracy: 0.5917 - val_loss: 0.6830\n",
            "Epoch 15/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6824 - loss: 0.6017 - val_accuracy: 0.5818 - val_loss: 0.6860\n",
            "Epoch 16/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6984 - loss: 0.5878 - val_accuracy: 0.5903 - val_loss: 0.6875\n",
            "Epoch 17/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6979 - loss: 0.5847 - val_accuracy: 0.5818 - val_loss: 0.6862\n",
            "Epoch 18/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6823 - loss: 0.5907 - val_accuracy: 0.5775 - val_loss: 0.6946\n",
            "Epoch 19/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7208 - loss: 0.5746 - val_accuracy: 0.5761 - val_loss: 0.6915\n",
            "Epoch 20/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7124 - loss: 0.5673 - val_accuracy: 0.5704 - val_loss: 0.7003\n",
            "Epoch 21/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7309 - loss: 0.5741 - val_accuracy: 0.5818 - val_loss: 0.7006\n",
            "Epoch 22/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7262 - loss: 0.5576 - val_accuracy: 0.5846 - val_loss: 0.7106\n",
            "Epoch 23/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7334 - loss: 0.5442 - val_accuracy: 0.5917 - val_loss: 0.7094\n",
            "Epoch 24/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7236 - loss: 0.5527 - val_accuracy: 0.5832 - val_loss: 0.7083\n",
            "Epoch 25/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7547 - loss: 0.5189 - val_accuracy: 0.5775 - val_loss: 0.7188\n",
            "Epoch 26/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7508 - loss: 0.5348 - val_accuracy: 0.5775 - val_loss: 0.7297\n",
            "Epoch 27/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7506 - loss: 0.5290 - val_accuracy: 0.5974 - val_loss: 0.7293\n",
            "Epoch 28/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7522 - loss: 0.5253 - val_accuracy: 0.5989 - val_loss: 0.7320\n",
            "Epoch 29/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7631 - loss: 0.5048 - val_accuracy: 0.5917 - val_loss: 0.7456\n",
            "Epoch 30/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7692 - loss: 0.5038 - val_accuracy: 0.5946 - val_loss: 0.7456\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5867 - loss: 0.7486 \n",
            "Final Test Accuracy for MLP: 59.46%\n",
            "\n",
            "Training Model GRU on Part 5 of dataset\n",
            "Epoch 1/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 681ms/step - accuracy: 0.3084 - loss: 1.7978 - val_accuracy: 0.4353 - val_loss: 1.2983\n",
            "Epoch 2/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 626ms/step - accuracy: 0.4809 - loss: 1.2009 - val_accuracy: 0.5306 - val_loss: 0.7737\n",
            "Epoch 3/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 596ms/step - accuracy: 0.5512 - loss: 0.8426 - val_accuracy: 0.5576 - val_loss: 0.7260\n",
            "Epoch 4/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 607ms/step - accuracy: 0.5709 - loss: 0.7837 - val_accuracy: 0.5846 - val_loss: 0.7011\n",
            "Epoch 5/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 615ms/step - accuracy: 0.5701 - loss: 0.7469 - val_accuracy: 0.5889 - val_loss: 0.6902\n",
            "Epoch 6/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 618ms/step - accuracy: 0.5682 - loss: 0.7244 - val_accuracy: 0.5747 - val_loss: 0.6834\n",
            "Epoch 7/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 619ms/step - accuracy: 0.5660 - loss: 0.7391 - val_accuracy: 0.5519 - val_loss: 0.6828\n",
            "Epoch 8/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 616ms/step - accuracy: 0.5275 - loss: 0.8407 - val_accuracy: 0.4566 - val_loss: 1.1049\n",
            "Epoch 9/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 645ms/step - accuracy: 0.4162 - loss: 1.1022 - val_accuracy: 0.4538 - val_loss: 0.9856\n",
            "Epoch 10/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 596ms/step - accuracy: 0.4450 - loss: 1.0590 - val_accuracy: 0.4737 - val_loss: 1.0579\n",
            "Epoch 11/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 612ms/step - accuracy: 0.4615 - loss: 1.0553 - val_accuracy: 0.5590 - val_loss: 0.7904\n",
            "Epoch 12/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 611ms/step - accuracy: 0.5151 - loss: 0.8592 - val_accuracy: 0.5533 - val_loss: 0.7740\n",
            "Epoch 13/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 609ms/step - accuracy: 0.5506 - loss: 0.7905 - val_accuracy: 0.5292 - val_loss: 0.6995\n",
            "Epoch 14/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 638ms/step - accuracy: 0.5690 - loss: 0.7117 - val_accuracy: 0.5932 - val_loss: 0.6841\n",
            "Epoch 15/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 611ms/step - accuracy: 0.5475 - loss: 0.7199 - val_accuracy: 0.5747 - val_loss: 0.6914\n",
            "Epoch 16/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 593ms/step - accuracy: 0.5710 - loss: 0.7024 - val_accuracy: 0.5789 - val_loss: 0.6768\n",
            "Epoch 17/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 588ms/step - accuracy: 0.5727 - loss: 0.6923 - val_accuracy: 0.5491 - val_loss: 0.6794\n",
            "Epoch 18/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 614ms/step - accuracy: 0.5794 - loss: 0.6888 - val_accuracy: 0.5405 - val_loss: 0.6779\n",
            "Epoch 19/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 615ms/step - accuracy: 0.5766 - loss: 0.6858 - val_accuracy: 0.5576 - val_loss: 0.6759\n",
            "Epoch 20/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 614ms/step - accuracy: 0.5613 - loss: 0.7007 - val_accuracy: 0.5704 - val_loss: 0.6740\n",
            "Epoch 21/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 615ms/step - accuracy: 0.5646 - loss: 0.7140 - val_accuracy: 0.5775 - val_loss: 0.6822\n",
            "Epoch 22/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 593ms/step - accuracy: 0.5588 - loss: 0.7196 - val_accuracy: 0.5775 - val_loss: 0.6748\n",
            "Epoch 23/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 612ms/step - accuracy: 0.5708 - loss: 0.6885 - val_accuracy: 0.5889 - val_loss: 0.6728\n",
            "Epoch 24/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 614ms/step - accuracy: 0.5714 - loss: 0.6853 - val_accuracy: 0.6117 - val_loss: 0.6697\n",
            "Epoch 25/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 614ms/step - accuracy: 0.5516 - loss: 0.7043 - val_accuracy: 0.5889 - val_loss: 0.6742\n",
            "Epoch 26/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 612ms/step - accuracy: 0.5628 - loss: 0.6925 - val_accuracy: 0.5775 - val_loss: 0.6740\n",
            "Epoch 27/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 617ms/step - accuracy: 0.5705 - loss: 0.6918 - val_accuracy: 0.6102 - val_loss: 0.6834\n",
            "Epoch 28/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 658ms/step - accuracy: 0.5667 - loss: 0.6840 - val_accuracy: 0.5533 - val_loss: 0.6738\n",
            "Epoch 29/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 616ms/step - accuracy: 0.5699 - loss: 0.6925 - val_accuracy: 0.5420 - val_loss: 0.6759\n",
            "Epoch 30/30\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 618ms/step - accuracy: 0.5627 - loss: 0.6912 - val_accuracy: 0.5889 - val_loss: 0.6801\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.5983 - loss: 0.6707\n",
            "Final Test Accuracy for GRU: 58.89%\n",
            "\n",
            "Best performing model: CNN-LSTM with accuracy 60.17%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, GRU, Dense, Dropout, Flatten, BatchNormalization, MultiHeadAttention, Input\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Dataset\n",
        "file_path = \"cryptography_dataset_enhanced.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=[\"Key\"]).reset_index(drop=True)\n",
        "\n",
        "# Encode Algorithm labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Algorithm_Label\"] = label_encoder.fit_transform(df[\"Algorithm\"])\n",
        "\n",
        "# Convert Ciphertext to fixed-length ASCII representation\n",
        "def text_to_ascii(text, max_length=128):\n",
        "    ascii_vals = [ord(c) for c in text[:max_length]]\n",
        "    if len(ascii_vals) < max_length:\n",
        "        ascii_vals.extend([0] * (max_length - len(ascii_vals)))\n",
        "    return np.array(ascii_vals)\n",
        "\n",
        "ciphertext_numeric = np.array([text_to_ascii(text, 128) for text in df[\"Ciphertext\"]])\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "ciphertext_scaled = scaler.fit_transform(ciphertext_numeric)\n",
        "\n",
        "# Reshape for CNN/LSTM input\n",
        "X = ciphertext_scaled.reshape(ciphertext_scaled.shape[0], 128, 1)\n",
        "y = df[\"Algorithm_Label\"].values\n",
        "\n",
        "# Split dataset into five parts\n",
        "split_size = len(df) // 5\n",
        "X_splits = [X[i * split_size:(i + 1) * split_size] for i in range(5)]\n",
        "y_splits = [y[i * split_size:(i + 1) * split_size] for i in range(5)]\n",
        "\n",
        "# Define improved models with attention and batch normalization\n",
        "def build_cnn_lstm():\n",
        "    inputs = Input(shape=(128, 1))\n",
        "    x = Conv1D(128, kernel_size=5, activation='relu')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = LSTM(128, return_sequences=True)(x)\n",
        "    x = MultiHeadAttention(num_heads=2, key_dim=64)(x, x)\n",
        "    x = LSTM(64)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(len(label_encoder.classes_), activation='softmax')(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "def build_lstm():\n",
        "    inputs = Input(shape=(128, 1))\n",
        "    x = LSTM(128, return_sequences=True)(inputs)\n",
        "    x = MultiHeadAttention(num_heads=2, key_dim=64)(x, x)\n",
        "    x = LSTM(64)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(len(label_encoder.classes_), activation='softmax')(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "def build_cnn():\n",
        "    model = Sequential([\n",
        "        Conv1D(128, kernel_size=5, activation='relu', input_shape=(128, 1)),\n",
        "        BatchNormalization(),\n",
        "        Conv1D(64, kernel_size=3, activation='relu'),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_mlp():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(128, 1)),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_gru():\n",
        "    inputs = Input(shape=(128, 1))\n",
        "    x = GRU(128, return_sequences=True)(inputs)\n",
        "    x = MultiHeadAttention(num_heads=2, key_dim=64)(x, x)\n",
        "    x = GRU(64)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(len(label_encoder.classes_), activation='softmax')(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "models = {\n",
        "    \"CNN-LSTM\": build_cnn_lstm(),\n",
        "    \"LSTM\": build_lstm(),\n",
        "    \"CNN\": build_cnn(),\n",
        "    \"MLP\": build_mlp(),\n",
        "    \"GRU\": build_gru()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model with improved settings\n",
        "epochs = 30\n",
        "batch_size = 64\n",
        "results = {}\n",
        "\n",
        "for i, (X_part, y_part) in enumerate(zip(X_splits, y_splits)):\n",
        "    print(f\"\\nTraining Model {list(models.keys())[i]} on Part {i+1} of dataset\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_part, y_part, test_size=0.2, random_state=42)\n",
        "\n",
        "    model_name = list(models.keys())[i]\n",
        "    model = models[model_name]\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "    results[model_name] = test_acc\n",
        "    print(f'Final Test Accuracy for {model_name}: {test_acc * 100:.2f}%')\n",
        "\n",
        "# Display best model\n",
        "best_model = max(results, key=results.get)\n",
        "print(f'\\nBest performing model: {best_model} with accuracy {results[best_model] * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Complement Naive Bayes"
      ],
      "metadata": {
        "id": "gEHL07WJL39D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIOubudOFql_",
        "outputId": "209a0d30-aaef-419a-ddad-a9bb67bede7b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training J48 (Decision Tree)...\n",
            "Final Test Accuracy for J48 (Decision Tree): 57.35%\n",
            "\n",
            "Training FT (Functional Trees)...\n",
            "Final Test Accuracy for FT (Functional Trees): 57.78%\n",
            "\n",
            "Training PART (Rule-based Classifier)...\n",
            "Final Test Accuracy for PART (Rule-based Classifier): 57.27%\n",
            "\n",
            "Training Complement Naive Bayes...\n",
            "Final Test Accuracy for Complement Naive Bayes: 57.81%\n",
            "\n",
            "Training Multilayer Perceptron...\n",
            "Epoch 1/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5149 - loss: 1.0119 - val_accuracy: 0.5764 - val_loss: 0.6671\n",
            "Epoch 2/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5892 - loss: 0.6820 - val_accuracy: 0.5735 - val_loss: 0.6637\n",
            "Epoch 3/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5872 - loss: 0.6778 - val_accuracy: 0.5670 - val_loss: 0.6637\n",
            "Epoch 4/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5990 - loss: 0.6700 - val_accuracy: 0.5807 - val_loss: 0.6670\n",
            "Epoch 5/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6120 - loss: 0.6597 - val_accuracy: 0.5710 - val_loss: 0.6636\n",
            "Epoch 6/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6139 - loss: 0.6570 - val_accuracy: 0.5676 - val_loss: 0.6659\n",
            "Epoch 7/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6155 - loss: 0.6608 - val_accuracy: 0.5627 - val_loss: 0.6708\n",
            "Epoch 8/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6381 - loss: 0.6517 - val_accuracy: 0.5684 - val_loss: 0.6700\n",
            "Epoch 9/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6359 - loss: 0.6493 - val_accuracy: 0.5730 - val_loss: 0.6711\n",
            "Epoch 10/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6382 - loss: 0.6457 - val_accuracy: 0.5755 - val_loss: 0.6709\n",
            "Epoch 11/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6524 - loss: 0.6409 - val_accuracy: 0.5698 - val_loss: 0.6790\n",
            "Epoch 12/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6515 - loss: 0.6308 - val_accuracy: 0.5704 - val_loss: 0.6860\n",
            "Epoch 13/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6606 - loss: 0.6227 - val_accuracy: 0.5716 - val_loss: 0.6821\n",
            "Epoch 14/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6672 - loss: 0.6201 - val_accuracy: 0.5730 - val_loss: 0.6848\n",
            "Epoch 15/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6674 - loss: 0.6132 - val_accuracy: 0.5727 - val_loss: 0.6906\n",
            "Epoch 16/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6768 - loss: 0.6113 - val_accuracy: 0.5716 - val_loss: 0.6969\n",
            "Epoch 17/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6877 - loss: 0.6084 - val_accuracy: 0.5747 - val_loss: 0.7027\n",
            "Epoch 18/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6873 - loss: 0.6026 - val_accuracy: 0.5670 - val_loss: 0.7177\n",
            "Epoch 19/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6905 - loss: 0.5929 - val_accuracy: 0.5684 - val_loss: 0.7147\n",
            "Epoch 20/20\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7154 - loss: 0.5754 - val_accuracy: 0.5653 - val_loss: 0.7203\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5608 - loss: 0.7190\n",
            "Final Test Accuracy for Multilayer Perceptron: 56.53%\n",
            "\n",
            "Best performing model: Complement Naive Bayes with accuracy 57.81%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Dataset\n",
        "file_path = \"cryptography_dataset_enhanced.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=[\"Key\"]).reset_index(drop=True)\n",
        "\n",
        "# Encode Algorithm labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Algorithm_Label\"] = label_encoder.fit_transform(df[\"Algorithm\"])\n",
        "\n",
        "# Convert Ciphertext to fixed-length ASCII representation\n",
        "def text_to_ascii(text, max_length=128):\n",
        "    ascii_vals = [ord(c) for c in text[:max_length]]\n",
        "    if len(ascii_vals) < max_length:\n",
        "        ascii_vals.extend([0] * (max_length - len(ascii_vals)))\n",
        "    return np.array(ascii_vals)\n",
        "\n",
        "ciphertext_numeric = np.array([text_to_ascii(text, 128) for text in df[\"Ciphertext\"]])\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "ciphertext_scaled = scaler.fit_transform(ciphertext_numeric)\n",
        "\n",
        "# Prepare input and labels\n",
        "X = ciphertext_scaled\n",
        "y = df[\"Algorithm_Label\"].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a separate dataset for ComplementNB (ensuring non-negative values)\n",
        "minmax_scaler = MinMaxScaler()\n",
        "X_train_nb = minmax_scaler.fit_transform(X_train)\n",
        "X_test_nb = minmax_scaler.transform(X_test)\n",
        "\n",
        "# Define classifiers\n",
        "models = {\n",
        "    \"J48 (Decision Tree)\": DecisionTreeClassifier(),\n",
        "    \"FT (Functional Trees)\": ExtraTreesClassifier(n_estimators=100),\n",
        "    \"PART (Rule-based Classifier)\": DecisionTreeClassifier(splitter='random'),\n",
        "    \"Complement Naive Bayes\": ComplementNB(),\n",
        "    \"Multilayer Perceptron\": Sequential([\n",
        "        Flatten(input_shape=(128,)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax')\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "    if model_name == \"Multilayer Perceptron\":\n",
        "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
        "        test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "        results[model_name] = test_acc\n",
        "    elif model_name == \"Complement Naive Bayes\":\n",
        "        model.fit(X_train_nb, y_train)\n",
        "        y_pred = model.predict(X_test_nb)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        results[model_name] = acc\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        results[model_name] = acc\n",
        "\n",
        "    print(f'Final Test Accuracy for {model_name}: {results[model_name] * 100:.2f}%')\n",
        "\n",
        "# Display best model\n",
        "best_model = max(results, key=results.get)\n",
        "print(f'\\nBest performing model: {best_model} with accuracy {results[best_model] * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN-LSTM model"
      ],
      "metadata": {
        "id": "xehradsRLuyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xScaVtAwTt25",
        "outputId": "a2d70745-2deb-4ea9-caf9-bb7c9a189e2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 858ms/step - accuracy: 0.1413 - loss: 1.9467 - val_accuracy: 0.1474 - val_loss: 1.9455\n",
            "Epoch 2/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 716ms/step - accuracy: 0.1412 - loss: 1.9462 - val_accuracy: 0.1511 - val_loss: 1.9456\n",
            "Epoch 3/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 731ms/step - accuracy: 0.1448 - loss: 1.9463 - val_accuracy: 0.1445 - val_loss: 1.9453\n",
            "Epoch 4/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 715ms/step - accuracy: 0.1466 - loss: 1.9462 - val_accuracy: 0.1405 - val_loss: 1.9453\n",
            "Epoch 5/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 725ms/step - accuracy: 0.1479 - loss: 1.9457 - val_accuracy: 0.1528 - val_loss: 1.9448\n",
            "Epoch 6/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 726ms/step - accuracy: 0.1494 - loss: 1.9458 - val_accuracy: 0.1528 - val_loss: 1.9449\n",
            "Epoch 7/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 727ms/step - accuracy: 0.1418 - loss: 1.9460 - val_accuracy: 0.1343 - val_loss: 1.9452\n",
            "Epoch 8/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 722ms/step - accuracy: 0.1437 - loss: 1.9454 - val_accuracy: 0.1459 - val_loss: 1.9445\n",
            "Epoch 9/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 728ms/step - accuracy: 0.1415 - loss: 1.9461 - val_accuracy: 0.1545 - val_loss: 1.9445\n",
            "Epoch 10/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 730ms/step - accuracy: 0.1444 - loss: 1.9456 - val_accuracy: 0.1531 - val_loss: 1.9443\n",
            "Epoch 11/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 731ms/step - accuracy: 0.1515 - loss: 1.9447 - val_accuracy: 0.1545 - val_loss: 1.9438\n",
            "Epoch 12/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 731ms/step - accuracy: 0.1496 - loss: 1.9446 - val_accuracy: 0.1525 - val_loss: 1.9427\n",
            "Epoch 13/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 746ms/step - accuracy: 0.1508 - loss: 1.9440 - val_accuracy: 0.1434 - val_loss: 1.9434\n",
            "Epoch 14/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 726ms/step - accuracy: 0.1419 - loss: 1.9448 - val_accuracy: 0.1616 - val_loss: 1.9391\n",
            "Epoch 15/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 718ms/step - accuracy: 0.1470 - loss: 1.9416 - val_accuracy: 0.1616 - val_loss: 1.9393\n",
            "Epoch 16/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 721ms/step - accuracy: 0.1440 - loss: 1.9446 - val_accuracy: 0.1496 - val_loss: 1.9395\n",
            "Epoch 17/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 722ms/step - accuracy: 0.1546 - loss: 1.9428 - val_accuracy: 0.1616 - val_loss: 1.9390\n",
            "Epoch 18/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 719ms/step - accuracy: 0.1564 - loss: 1.9423 - val_accuracy: 0.1616 - val_loss: 1.9389\n",
            "Epoch 19/50\n",
            "\u001b[1m122/220\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 674ms/step - accuracy: 0.1535 - loss: 1.9441"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Flatten, BatchNormalization, Embedding\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load Dataset\n",
        "file_path = \"cryptography_dataset_enhanced.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=[\"Key\"]).reset_index(drop=True)\n",
        "\n",
        "# Encode Algorithm labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Algorithm_Label\"] = label_encoder.fit_transform(df[\"Algorithm\"])\n",
        "\n",
        "# Convert Ciphertext using TF-IDF instead of ASCII encoding\n",
        "vectorizer = TfidfVectorizer(max_features=128)\n",
        "X_tfidf = vectorizer.fit_transform(df[\"Ciphertext\"]).toarray()\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_tfidf)\n",
        "\n",
        "# Reshape for CNN-LSTM input\n",
        "X = X_scaled.reshape(X_scaled.shape[0], 128, 1)\n",
        "y = df[\"Algorithm_Label\"].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define improved CNN-LSTM model\n",
        "def build_cnn_lstm():\n",
        "    model = Sequential([\n",
        "        Conv1D(256, kernel_size=5, activation='relu', input_shape=(128, 1)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Conv1D(128, kernel_size=3, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        LSTM(128, return_sequences=True, recurrent_dropout=0.2),\n",
        "        LSTM(64, recurrent_dropout=0.2),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Build and train the model\n",
        "model = build_cnn_lstm()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with optimized settings\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Final Test Accuracy: {test_acc * 100:.2f}%')\n",
        ".........."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN-BiLSTM model"
      ],
      "metadata": {
        "id": "33AvCvneLoXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "089xnq-fUogS",
        "outputId": "ceacc58d-5dac-4426-b696-ca07b16bb5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m751s\u001b[0m 3s/step - accuracy: 0.1421 - loss: 2.1843 - val_accuracy: 0.1431 - val_loss: 2.0528\n",
            "Epoch 2/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m742s\u001b[0m 3s/step - accuracy: 0.1444 - loss: 2.0301 - val_accuracy: 0.1496 - val_loss: 1.9795\n",
            "Epoch 3/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m726s\u001b[0m 3s/step - accuracy: 0.1493 - loss: 1.9727 - val_accuracy: 0.1499 - val_loss: 1.9537\n",
            "Epoch 4/50\n",
            "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 3s/step - accuracy: 0.1451 - loss: 1.9553 - val_accuracy: 0.1607 - val_loss: 1.9469\n",
            "Epoch 5/50\n",
            "\u001b[1m212/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.1521 - loss: 1.9485"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, BatchNormalization, Bidirectional, Flatten, Input\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load Dataset\n",
        "file_path = \"cryptography_dataset_enhanced.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=[\"Key\"]).reset_index(drop=True)\n",
        "\n",
        "# Encode Algorithm labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Algorithm_Label\"] = label_encoder.fit_transform(df[\"Algorithm\"])\n",
        "\n",
        "# Convert Ciphertext using TF-IDF with more features\n",
        "vectorizer = TfidfVectorizer(max_features=256)  # Increased features\n",
        "X_tfidf = vectorizer.fit_transform(df[\"Ciphertext\"]).toarray()\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_tfidf)\n",
        "\n",
        "# Reshape for CNN-BiLSTM input\n",
        "X = X_scaled.reshape(X_scaled.shape[0], 256, 1)\n",
        "y = df[\"Algorithm_Label\"].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define optimized CNN-BiLSTM model\n",
        "def build_optimized_model():\n",
        "    model = Sequential([\n",
        "        Input(shape=(256, 1)),\n",
        "        Conv1D(256, kernel_size=5, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.2)),\n",
        "        Bidirectional(LSTM(64, recurrent_dropout=0.2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(label_encoder.classes_), activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Build and compile the model with RMSprop optimizer\n",
        "model = build_optimized_model()\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0005, clipnorm=1.0),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with optimized settings\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Final Test Accuracy: {test_acc * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random-Forest-Classifier"
      ],
      "metadata": {
        "id": "vyJJWvfILX1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import base64\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"cryptography_dataset_enhanced (1).csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop unnecessary columns (Plaintext and Key are irrelevant for classification)\n",
        "df = df.drop(columns=[\"Plaintext\", \"Key\"])\n",
        "\n",
        "# Function to extract byte frequency histogram from Ciphertext\n",
        "def extract_histogram(ciphertext):\n",
        "    try:\n",
        "        decoded_bytes = base64.b64decode(ciphertext, validate=True)  # Convert Base64 to bytes\n",
        "    except:\n",
        "        decoded_bytes = ciphertext.encode()  # If not Base64, use direct encoding\n",
        "\n",
        "    histogram = np.zeros(256)  # Initialize histogram with 256 bins (0-255 byte values)\n",
        "    for byte in decoded_bytes:\n",
        "        histogram[byte] += 1  # Count occurrences of each byte\n",
        "    return histogram\n",
        "\n",
        "# Apply histogram extraction to all Ciphertext entries\n",
        "histograms = np.array([extract_histogram(ct) for ct in df[\"Ciphertext\"]])\n",
        "\n",
        "# Convert to DataFrame\n",
        "feature_columns = [f\"Byte_{i}\" for i in range(256)]\n",
        "hist_df = pd.DataFrame(histograms, columns=feature_columns)\n",
        "\n",
        "# Add the Algorithm labels\n",
        "hist_df[\"Algorithm\"] = df[\"Algorithm\"]\n",
        "\n",
        "# Split dataset into features (X) and labels (y)\n",
        "X = hist_df.drop(columns=[\"Algorithm\"])\n",
        "y = hist_df[\"Algorithm\"]\n",
        "\n",
        "# Split into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=200, random_state=42)  # Increased trees to improve accuracy\n",
        "print(\"\\nTraining the improved model...\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nImproved Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEnqeYTkNzFZ",
        "outputId": "5e1c9737-1d1d-4869-dca8-602ecf464510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the improved model...\n",
            "\n",
            "Improved Model Accuracy: 0.41\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        3DES       0.23      0.19      0.21        67\n",
            "         AES       0.40      0.34      0.37        67\n",
            "    Blowfish       0.12      0.09      0.10        66\n",
            "    ChaCha20       0.16      0.29      0.21        48\n",
            "         DES       0.11      0.08      0.09        52\n",
            "         RC4       0.24      0.27      0.25        77\n",
            "         RSA       1.00      1.00      1.00        68\n",
            "     SHA-256       0.95      1.00      0.97        55\n",
            "\n",
            "    accuracy                           0.41       500\n",
            "   macro avg       0.40      0.41      0.40       500\n",
            "weighted avg       0.40      0.41      0.40       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8xoosNHOsvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}